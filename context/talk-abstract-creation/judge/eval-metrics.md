# Talk Abstract Evaluation Metrics

## Overview
This document defines measurable metrics for evaluating talk abstract quality, professional standards, and alignment with Dominik Schmidle's writing style. Each metric is scored 0-10, with specific criteria for different score levels.

---

## Category 1: Style Authenticity (40 points total)

### 1.1 Voice Consistency (10 points)
**Measures:** How closely the writing matches Dominik's authentic voice

**Scoring Rubric:**
- **9-10 (Authentic):** Could not be distinguished from Dominik's own writing. Natural flow, characteristic phrases, typical sentence structures
- **7-8 (Very Close):** Minor deviations in phrasing or rhythm. Feels mostly authentic with 1-2 "off" moments
- **5-6 (Recognizable):** Core voice is there but feels slightly "AI-polished" or missing personality edges
- **3-4 (Generic):** Reads like standard tech writing, loses distinctive voice
- **0-2 (Off-Brand):** Does not sound like Dominik at all

**Key Markers to Check:**
- [ ] Conversational, not corporate
- [ ] Authentic enthusiasm without hype
- [ ] Personal pronouns used naturally (we, I, our)
- [ ] No excessive buzzword stacking
- [ ] Sentence variety (short punchy + longer explanatory)

---

### 1.2 Tone Alignment (10 points)
**Measures:** Emotional and professional tone matches target style

**Scoring Rubric:**
- **9-10 (Perfect Match):** Exact balance of professional + approachable + humble + enthusiastic
- **7-8 (Strong):** Tone is appropriate with minor imbalances (slightly too formal OR slightly too casual)
- **5-6 (Acceptable):** Recognizably professional but missing personality OR too casual for conference setting
- **3-4 (Misaligned):** Too corporate/stiff OR too informal/unprofessional
- **0-2 (Wrong):** Completely inappropriate tone (e.g., sales-pitchy, arrogant, overly academic)

**Key Markers to Check:**
- [ ] Honest about challenges (not just success-porn)
- [ ] Confident without arrogance
- [ ] Optimistic and solutions-focused
- [ ] Self-aware (acknowledges limitations or learnings)
- [ ] Professional but warm

---

### 1.3 Language Patterns (10 points)
**Measures:** Use of characteristic vocabulary, phrases, and structures

**Scoring Rubric:**
- **9-10 (Signature Style):** Uses Dominik's characteristic phrases, structures, and vocabulary naturally
- **7-8 (Familiar):** Language feels right, maybe 1 phrase that's not typical
- **5-6 (Generic Tech):** Correct but generic tech industry language
- **3-4 (Bland):** Overly formal or template-like language
- **0-2 (Wrong):** Language patterns completely foreign to style

**Characteristic Elements to Check:**
- [ ] Technical terms used correctly and naturally (not forced)
- [ ] Transformation narratives ("from X to Y")
- [ ] Real-world framing ("in practice", "the reality is", "here's what happened")
- [ ] Avoids: "leverage", "synergy", "best-in-class", "cutting-edge", "game-changer"
- [ ] Prefers: "practical", "real-world", "journey", "patterns", "lessons learned"

---

### 1.4 Structural Flow (10 points)
**Measures:** Organization and progression matches typical patterns

**Scoring Rubric:**
- **9-10 (Natural Flow):** Perfect narrative arc, easy to follow, engaging progression
- **7-8 (Good Structure):** Clear structure with minor pacing issues
- **5-6 (Adequate):** Gets the point across but feels mechanical
- **3-4 (Choppy):** Jumpy or disconnected ideas
- **0-2 (Confused):** No clear structure or logic flow

**Structural Elements to Check:**
- [ ] Opens with relatable problem or context
- [ ] Builds through transformation/journey
- [ ] Includes concrete examples/metrics mid-way
- [ ] Ends with value proposition or takeaways
- [ ] Uses transitions naturally (not "Additionally", "Furthermore", "Moreover")

---

## Category 2: Content Quality (35 points total)

### 2.1 Narrative Strength (10 points)
**Measures:** Compelling story arc and transformation journey

**Scoring Rubric:**
- **9-10 (Compelling):** Clear transformation with emotional resonance, makes you want to attend
- **7-8 (Engaging):** Good story with clear arc, minor gaps in narrative
- **5-6 (Present):** Story exists but feels shallow or rushed
- **3-4 (Weak):** Barely a narrative, mostly facts
- **0-2 (Missing):** No story element at all

**Narrative Elements to Check:**
- [ ] Clear "before" state (the problem/chaos)
- [ ] Transformation journey (what changed)
- [ ] "After" state (the outcome)
- [ ] Unexpected elements or learnings
- [ ] Human elements (not just technical)

---

### 2.2 Concrete Specificity (10 points)
**Measures:** Use of real numbers, specific examples, and tangible details

**Scoring Rubric:**
- **9-10 (Highly Specific):** Multiple concrete metrics, specific technologies, real examples (e.g., "150+ clusters", "5-person team", "10+ customers", "multiple continents")
- **7-8 (Good Detail):** 2-3 concrete specifics, but could use more
- **5-6 (Some Detail):** 1 specific metric or example, rest is vague
- **3-4 (Vague):** Generic claims without specifics
- **0-2 (Abstract):** No concrete details at all

**Specificity to Check:**
- [ ] Provable numbers included (team size, customer count, cluster count, geographic reach)
- [ ] Specific technologies/tools named (not "monitoring tools" but "LGTM stack")
- [ ] Concrete scale stated (how many clusters, users, services)
- [ ] Real timeframes mentioned ("two years", "started small")
- [ ] Specific context (industry, constraints, starting conditions)

**Good Specifics (Easily Provable):**
- ✅ Team size: "5-person team", "3 engineers"
- ✅ Scale: "150+ clusters", "hundreds of developers", "10+ customers"
- ✅ Geography: "multiple continents", "across 5 countries"
- ✅ Time: "two years ago", "18-month journey"
- ✅ Tech stack: "LGTM stack", "Kubernetes", "Terraform"

**Avoid (Hard to Prove/Sounds Like Bragging):**
- ❌ Percentage improvements without baseline: "90% reduction", "10x faster"
- ❌ Vague comparisons: "significantly better", "much improved"
- ❌ Unverifiable claims: "industry-leading", "best performance"

**⚠️ CRITICAL: Never Invent Specifics**
- ❌ **NEVER make up numbers** if not provided in context
- ❌ **NEVER fabricate metrics** to make abstract sound better
- ✅ **ALWAYS ask for missing information** if specifics would improve the abstract
- ✅ If real numbers aren't available, describe the situation qualitatively instead

**Example:**
- ❌ BAD: Inventing "150+ clusters" when you don't know the actual number
- ✅ GOOD: Asking "How many clusters does the platform manage?" or using "large-scale deployment" if number unknown

---

### 2.3 Value Clarity (10 points)
**Measures:** Clear articulation of what attendees will learn/gain

**Scoring Rubric:**
- **9-10 (Crystal Clear):** Immediately obvious who this is for and what they'll get
- **7-8 (Clear):** Value is stated but could be more specific
- **5-6 (Implied):** Reader has to infer the value
- **3-4 (Unclear):** Hard to tell what the takeaway is
- **0-2 (Missing):** No clear value proposition

**Value Elements to Check:**
- [ ] Target audience explicitly identified
- [ ] Specific learnings/takeaways mentioned
- [ ] Actionable outcomes promised
- [ ] Clear differentiation (why THIS talk vs others)
- [ ] Transferable patterns/frameworks offered

---

### 2.4 Technical Credibility (5 points)
**Measures:** Technical accuracy and appropriate depth signals

**Scoring Rubric:**
- **5 (Expert Level):** Technical terms used precisely, depth appropriate for topic, credible claims
- **4 (Solid):** Technically sound with minor imprecisions
- **3 (Adequate):** Correct but shallow or overly simplified
- **2 (Questionable):** Technical claims seem off or overstated
- **0-1 (Wrong):** Technical errors or misnomers

**Technical Elements to Check:**
- [ ] Technology names correct (Kubernetes not K8s in formal abstract)
- [ ] Architectural concepts accurate (multi-tenancy, RBAC)
- [ ] Scale claims realistic and specific
- [ ] Tool/stack names precise (LGTM, not "Grafana stack")
- [ ] Avoids impossible claims or hype

---

## Category 3: Professional Standards (25 points total)

### 3.1 Conference Appropriateness (10 points)
**Measures:** Fit for professional conference submission standards

**Scoring Rubric:**
- **9-10 (Exemplary):** Perfect for conference program committees, compelling and professional
- **7-8 (Professional):** Meets standards, minor polish needed
- **5-6 (Acceptable):** Would pass review but not stand out
- **3-4 (Marginal):** Borderline professional quality
- **0-2 (Inappropriate):** Would likely be rejected

**Professional Elements to Check:**
- [ ] Third-person perspective (if required by conference)
- [ ] Appropriate length (within guidelines)
- [ ] No sales/marketing language
- [ ] Honest about scope (not overselling)
- [ ] Clear structure and organization
- [ ] Grammar and spelling perfect

---

### 3.2 Audience Targeting (8 points)
**Measures:** Clear identification of who should attend and why

**Scoring Rubric:**
- **8 (Laser-Focused):** Perfect audience targeting, clear who benefits and how
- **6-7 (Well-Targeted):** Good audience identification with minor ambiguity
- **4-5 (Broad):** Too broad or slightly unclear target
- **2-3 (Vague):** Unclear who this is for
- **0-1 (Missing):** No audience indication

**Targeting Elements to Check:**
- [ ] Specific roles mentioned (PMs, platform engineers, SREs)
- [ ] Experience level implied (struggling teams, scaling challenges)
- [ ] Problem alignment (what pain points this addresses)
- [ ] Clear exclusions (not for everyone)
- [ ] Outcome match (who benefits from these learnings)

---

### 3.3 Differentiation (7 points)
**Measures:** Clear distinction through unique perspective and value

**Scoring Rubric:**
- **7 (Highly Unique):** Clearly different angle, perspective, or approach through positive framing
- **5-6 (Differentiated):** Has unique elements that make it stand out
- **3-4 (Some Uniqueness):** Slight differences but not strongly highlighted
- **1-2 (Generic):** Could describe many talks
- **0 (Undifferentiated):** Completely generic

**Differentiation Elements to Check:**
- [ ] Unique perspective explicitly stated (PM view, organizational focus, multi-org patterns)
- [ ] Specific angle highlighted (not just "how we did X")
- [ ] Cross-domain insights emphasized (technical + organizational)
- [ ] Unexpected elements called out (revenue story, failures, scale paradox)
- [ ] Specific frameworks/patterns offered (not just general advice)

**How to Differentiate Without Bashing Others:**
- ✅ "From a Product Manager's perspective..."
- ✅ "Drawing from workshops with dozens of organizations..."
- ✅ "Focuses on organizational and product strategy"
- ✅ "This isn't a technical deep-dive but..."
- ❌ "Most talks fail to address..." (sounds dismissive)
- ❌ "Other talks are missing..." (sounds arrogant)
- ❌ "Unlike typical talks that only..." (sounds critical)

---

## Category 4: Impact Potential (Bonus - up to 10 points)

### 4.1 Memorability (5 points)
**Measures:** How likely this talk is to stick in people's minds

**Elements:**
- [ ] Provocative or surprising opening
- [ ] Memorable numbers or facts
- [ ] Honest vulnerability (failures, struggles)
- [ ] Unexpected outcomes or pivots
- [ ] Clear takeaway framework

**Scoring:**
- **5:** Will definitely be referenced in hallway conversations
- **3-4:** Solid content, might be remembered
- **1-2:** Forgettable despite being competent
- **0:** Generic, will blend into background

---

### 4.2 Community Value (5 points)
**Measures:** Benefit to broader ecosystem beyond just sharing experience

**Elements:**
- [ ] Transferable patterns offered
- [ ] Fills identified gap in community knowledge
- [ ] Helps others avoid mistakes
- [ ] Shares frameworks/tools/approaches
- [ ] Advances industry thinking

**Scoring:**
- **5:** High community value, fills real gap
- **3-4:** Useful contribution
- **1-2:** Primarily about speaker/company
- **0:** No clear community benefit

---

## Total Scoring Rubric

**Maximum Score: 110 points (100 base + 10 bonus)**

### Score Interpretation:
- **95-110 (Excellent):** Ready for submission, highly competitive, sounds authentically like Dominik
- **80-94 (Strong):** Very good, minor refinements needed
- **65-79 (Good):** Solid foundation, needs targeted improvements
- **50-64 (Adequate):** Needs significant work on style or content
- **35-49 (Weak):** Major revisions required
- **Below 35 (Poor):** Start over with different approach

---

## Usage Guidelines for AI Judges

### Step 1: Read Abstract Completely
- First impression without scoring
- Note gut reaction to voice/quality

### Step 2: Score Each Category
- Go through metrics systematically
- Use scoring rubrics, not gut feel
- Note specific examples for each score
- Mark which checklist items pass/fail

### Step 3: Calculate Totals
- Sum each category
- Calculate percentage
- Identify weakest areas

### Step 4: Provide Structured Feedback

**Format:**
```
TOTAL SCORE: XX/110 (XX%)
CATEGORY BREAKDOWN:
- Style Authenticity: XX/40
- Content Quality: XX/35
- Professional Standards: XX/25
- Impact Potential: XX/10

TOP STRENGTHS:
1. [Specific strength with example]
2. [Specific strength with example]

CRITICAL IMPROVEMENTS NEEDED:
1. [Specific issue with example and suggested fix]
2. [Specific issue with example and suggested fix]

STYLE ALIGNMENT:
[Assessment of how close to Dominik's voice, with specific examples]

RECOMMENDATION:
[Ready to submit / Needs refinement / Needs major revision]
```

---

## Calibration Examples

### Example Score: 92/110 (Excellent)
**Sample Abstract:** "From Alert Fatigue to Platform Product: Scaling Observability Across 150+ Kubernetes Clusters"

**Why This Scores High:**
- Voice Consistency (9/10): Natural, conversational, authentic
- Concrete Specificity (10/10): "150+ clusters", "alert fatigue", specific transformation
- Narrative Strength (10/10): Clear journey from problem to product
- Professional Standards (9/10): Perfect conference fit
- Style authentic, compelling story, clear value

**What Could Improve:**
- Could add one more specific metric (team size, timeframe)
- Slightly more explicit audience targeting

---

### Example Score: 68/110 (Good - Needs Work)
**Sample Abstract:** "Building Better Observability Through Platform Engineering"

**Why This Scores Lower:**
- Voice Consistency (5/10): Generic tech writing, not distinctive
- Concrete Specificity (4/10): No real metrics or examples
- Narrative Strength (5/10): No clear story arc
- Tone Alignment (6/10): Too formal, missing personality

**What Would Improve It:**
- Add specific transformation narrative
- Include concrete numbers and scale
- Make it personal and story-driven
- Identify clear audience and value
- Use more characteristic language

---

## Red Flags (Automatic Score Reductions)

### Voice/Style Red Flags (-5 to -10 points each):
- ❌ Uses "leverage", "synergy", "game-changing"
- ❌ Overly formal academic language
- ❌ Sales/marketing pitch tone
- ❌ No personality or human element
- ❌ Buzzword bingo (5+ buzzwords in abstract)

### Content Red Flags (-5 to -10 points each):
- ❌ No concrete metrics or specifics
- ❌ Vague claims without evidence
- ❌ Impossible or unrealistic claims
- ❌ Technical inaccuracies
- ❌ No clear value proposition
- ❌ **INVENTED SPECIFICS (-10 points):** Made-up numbers, fake metrics, or fabricated details not provided in context

### Professional Red Flags (-5 to -10 points each):
- ❌ Grammar or spelling errors
- ❌ Unclear target audience
- ❌ Generic (could describe any talk)
- ❌ Company promotional language
- ❌ Wrong perspective (1st person when 3rd required)

---

## Iteration Guidelines

After scoring, if score is below 85:

1. **Identify the lowest-scoring category**
2. **Focus improvements on that area first**
3. **Rewrite (don't patch) to maintain natural flow**
4. **Re-score after changes**
5. **Repeat until reaching 85+ or diminishing returns**

### Typical Iteration Pattern:
- First draft: 60-75 (foundation exists but needs style/content work)
- Second draft: 75-85 (content improved, style getting closer)
- Third draft: 85-95 (ready or near-ready)
- Fourth draft: 95+ (excellent, authentic, ready)

---

## Notes for Evaluators

1. **Be Specific:** Don't just say "improve the voice" - identify exactly which phrases feel off
2. **Use Examples:** Reference specific sentences or phrases when scoring
3. **Compare to Originals:** Keep talk-examples.md open for style comparison
4. **Consider Context:** Different conferences may need slightly different styles
5. **Holistic + Detailed:** Score metrics individually but also consider overall impression
6. **Calibrate Regularly:** Review Dominik's new talks to keep calibration current
7. **VERIFY SPECIFICS:** Check that all numbers/metrics were provided in context - if they seem invented, apply -10 point penalty and flag for requesting real data

---

*This eval framework is designed to be used by AI judges in an automated evaluation pipeline, providing consistent, measurable, and actionable feedback on talk abstract quality and style alignment.*

